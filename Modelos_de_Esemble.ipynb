{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é Boosting em Machine Learning\n",
    "\n",
    "**Boosting** é uma técnica de aprendizado de máquina que busca melhorar o desempenho de modelos de aprendizado fraco (modelos com baixo desempenho individual) combinando-os de maneira sequencial. O objetivo do boosting é criar um modelo forte a partir de múltiplos modelos fracos. Em vez de treinar os modelos de forma independente (como no bagging), o boosting treina modelos em sequência, onde cada novo modelo tenta corrigir os erros cometidos pelos modelos anteriores.\n",
    "\n",
    "### Como Funciona o Boosting:\n",
    "\n",
    "1. **Treinamento Inicial**:\n",
    "   - O processo começa com o treinamento de um modelo inicial em todo o conjunto de dados. Esse modelo pode ser qualquer modelo simples, como uma árvore de decisão rasa (também chamada de \"stump\").\n",
    "\n",
    "2. **Cálculo dos Erros**:\n",
    "   - O modelo é avaliado e os erros de classificação são identificados. A ideia é focar nos exemplos que foram mal classificados pelo modelo anterior.\n",
    "\n",
    "3. **Ajuste dos Pesos dos Exemplos**:\n",
    "   - Os exemplos mal classificados ganham mais peso, ou seja, se tornam mais importantes para o próximo modelo. O modelo subsequente será treinado de forma a corrigir esses erros.\n",
    "   \n",
    "4. **Treinamento de Modelos Subsequentes**:\n",
    "   - A sequência continua, com cada novo modelo ajustando-se para corrigir os erros do modelo anterior, focando nos exemplos mal classificados.\n",
    "\n",
    "5. **Combinação das Previsões**:\n",
    "   - As previsões finais são feitas combinando as previsões dos modelos individuais. O algoritmo de boosting geralmente utiliza uma **votação ponderada** ou **média ponderada** para decidir a previsão final.\n",
    "\n",
    "### Como o Boosting Melhora o Erro dos Modelos Anteriores:\n",
    "\n",
    "O boosting trabalha **aumentando os pesos dos exemplos mal classificados** no conjunto de dados. Isso faz com que o próximo modelo se concentre mais nesses exemplos difíceis. Ao corrigir erros anteriores, o modelo subsequente consegue reduzir o erro total do sistema.\n",
    "\n",
    "### Exemplo Simplificado:\n",
    "- Digamos que você tenha quatro exemplos e um modelo inicial que errou em dois deles. O próximo modelo se concentrará mais nesses dois exemplos, tentando corrigir seus erros.\n",
    "- Esse processo é repetido, e o erro geral é reduzido à medida que os modelos se concentram em corrigir os exemplos mal classificados.\n",
    "\n",
    "### Vantagens do Boosting:\n",
    "1. Reduz o **viés** do modelo, criando um modelo mais preciso.\n",
    "2. Aumenta o desempenho ao focar nos erros cometidos por modelos anteriores.\n",
    "3. Pode produzir modelos extremamente poderosos, como no caso do **AdaBoost** ou **Gradient Boosting**.\n",
    "\n",
    "---\n",
    "\n",
    "## O que é Bagging em Machine Learning\n",
    "\n",
    "**Bagging** (Bootstrap Aggregating) é uma técnica de aprendizado de máquina que visa melhorar o desempenho de modelos, principalmente reduzir o **overfitting** e aumentar a precisão, ao combinar vários modelos fracos em um modelo mais forte. A ideia central do bagging é **treinar múltiplos modelos de forma independente** e, em seguida, combinar suas previsões para formar uma decisão final.\n",
    "\n",
    "### Como Funciona o Bagging:\n",
    "\n",
    "1. **Criação de Subconjuntos de Dados (Bootstrap)**:\n",
    "   - Para cada modelo, o bagging começa criando um **subconjunto de dados** diferente a partir do conjunto de dados original, usando o método de **amostragem com reposição**.\n",
    "   - A amostragem com reposição significa que, ao selecionar exemplos para o subconjunto de dados, um mesmo exemplo pode ser selecionado várias vezes ou pode ser omitido. Em média, cerca de **63% dos dados originais** são usados em cada subconjunto, e os 37% restantes não são selecionados para aquele modelo.\n",
    "   \n",
    "2. **Treinamento de Modelos Independentes**:\n",
    "   - Após a criação dos subconjuntos de dados, o bagging treina um modelo independente para cada subconjunto. O tipo de modelo pode ser qualquer modelo de aprendizado de máquina, mas geralmente, utiliza-se **árvores de decisão**, pois elas tendem a ser instáveis (variam muito com pequenas mudanças nos dados) e, portanto, se beneficiam bastante do bagging.\n",
    "   \n",
    "3. **Combinação das Previsões**:\n",
    "   - Depois que todos os modelos são treinados, suas previsões são combinadas de maneira diferente, dependendo se o problema é de **classificação** ou **regressão**:\n",
    "     - **Classificação**: A previsão final é determinada pela **votação majoritária**. Ou seja, o rótulo predito é aquele que foi mais frequentemente escolhido pelos modelos individuais.\n",
    "     - **Regressão**: A previsão final é geralmente a **média** das previsões feitas por cada modelo.\n",
    "\n",
    "### Exemplo Simplificado:\n",
    "\n",
    "Vamos imaginar que você tem um conjunto de dados com 1000 exemplos, e você usa bagging para criar 5 modelos, com as seguintes etapas:\n",
    "\n",
    "1. **Primeiro Passo**: Criamos 5 subconjuntos de dados a partir do conjunto original (amostragem com reposição).\n",
    "   \n",
    "2. **Segundo Passo**: Treinamos um modelo para cada subconjunto de dados. Cada modelo aprende a partir de diferentes subconjuntos de dados, então eles não veem os mesmos dados.\n",
    "\n",
    "3. **Terceiro Passo**: Para prever um novo exemplo, cada um dos 5 modelos faz uma previsão. Se for um problema de **classificação**, a previsão final será a classe que a maioria dos modelos previu. Se for um problema de **regressão**, a previsão final será a média das saídas dos modelos.\n",
    "\n",
    "### Vantagens do Bagging:\n",
    "\n",
    "1. **Redução da Variância**:\n",
    "   - O bagging reduz a **variância** do modelo, ou seja, ele ajuda a evitar que o modelo se ajuste demais (overfit) aos dados de treinamento. Isso é especialmente útil para modelos como árvores de decisão, que podem ser muito sensíveis às flutuações nos dados.\n",
    "   \n",
    "2. **Melhora a Estabilidade do Modelo**:\n",
    "   - Como o bagging treina vários modelos em subconjuntos diferentes dos dados, ele faz com que o modelo final seja mais robusto, pois o erro de cada modelo individual é suavizado.\n",
    "\n",
    "3. **Aumento da Performance**:\n",
    "   - Ao combinar vários modelos, o bagging geralmente leva a um modelo final mais preciso e robusto do que qualquer modelo individual.\n",
    "\n",
    "4. **Paralelização**:\n",
    "   - O bagging pode ser facilmente paralelizado, já que os modelos são treinados de forma independente. Isso pode resultar em uma execução mais rápida, especialmente em grandes conjuntos de dados.\n",
    "\n",
    "### Desvantagens do Bagging:\n",
    "\n",
    "1. **Alta Complexidade Computacional**:\n",
    "   - Como o bagging envolve a construção de múltiplos modelos, ele pode ser computacionalmente caro, especialmente em grandes conjuntos de dados e quando muitos modelos são treinados.\n",
    "\n",
    "2. **Modelos Não Interpretáveis**:\n",
    "   - Quando você combina muitos modelos (como no bagging), o modelo final tende a ser mais difícil de interpretar, pois você perde a visibilidade sobre como os diferentes modelos estão tomando decisões.\n",
    "\n",
    "### Algoritmos Comuns que Usam Bagging:\n",
    "\n",
    "- **Random Forest**: Um dos algoritmos mais populares que utiliza bagging é o **Random Forest**, que é uma coleção de árvores de decisão treinadas com diferentes subconjuntos de dados. Além disso, no Random Forest, há também uma amostragem aleatória das características (features) em cada divisão de uma árvore, o que o torna mais robusto ainda.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de Bagging com `BaggingClassifier`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregando o conjunto de dados Iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Dividindo o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Criando o modelo base (neste caso, uma árvore de decisão)\n",
    "base_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Criando o modelo de Bagging com o modelo base\n",
    "bagging_model = BaggingClassifier(base_model, n_estimators=50, random_state=42)\n",
    "\n",
    "# Treinando o modelo\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Avaliando a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação do Código de Bagging com `BaggingClassifier`\n",
    "\n",
    "- **`DecisionTreeClassifier`**: O modelo base que será usado no Bagging. Neste exemplo, estamos utilizando uma árvore de decisão. Você pode usar qualquer outro classificador ou regressor como modelo base.\n",
    "\n",
    "- **`BaggingClassifier`**: A classe do scikit-learn que implementa a técnica de Bagging. Ela recebe o modelo base e treina múltiplos modelos com diferentes amostras dos dados. O parâmetro `n_estimators` define o número de modelos base que serão treinados.\n",
    "\n",
    "- **`fit(X_train, y_train)`**: Treina o modelo de Bagging no conjunto de treinamento, ou seja, a combinação dos modelos base.\n",
    "\n",
    "- **`predict(X_test)`**: Faz previsões usando os modelos treinados no conjunto de testes.\n",
    "\n",
    "- **`accuracy_score(y_test, y_pred)`**: Calcula a acurácia das previsões feitas, comparando as previsões `y_pred` com os valores reais `y_test`.\n",
    "\n",
    "### Parâmetros Importantes do `BaggingClassifier`:\n",
    "\n",
    "- **`n_estimators`**: Número de estimadores (modelos base) a ser treinado. O valor padrão é 10, mas pode ser ajustado conforme necessário.\n",
    "\n",
    "- **`max_samples`**: Define a fração ou o número de amostras a ser usado para treinar cada modelo base. O valor padrão é `1.0`, o que significa que cada modelo base será treinado com todas as amostras do conjunto de dados.\n",
    "\n",
    "- **`max_features`**: Define a fração ou o número de características (features) a ser utilizado para treinar cada modelo base.\n",
    "\n",
    "- **`bootstrap`**: Se `True`, realiza amostragem com reposição (bootstrap); se `False`, realiza amostragem sem reposição.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exemplo de Bagging com `BaggingRegressor` para Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.1326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Gerando um conjunto de dados de regressão\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Criando o modelo base (um regressor de árvore de decisão)\n",
    "base_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Criando o modelo de Bagging para regressão\n",
    "bagging_regressor = BaggingRegressor(base_regressor, n_estimators=50, random_state=42)\n",
    "\n",
    "# Treinando o modelo\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = bagging_regressor.predict(X_test)\n",
    "\n",
    "# Avaliando a performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O processo é similar, mas aqui estamos usando um **regressor de árvore de decisão** para prever valores contínuos em vez de classes:\n",
    "\n",
    "- **`BaggingRegressor`**: A classe para Bagging em problemas de regressão. Ela combina vários regressores base para melhorar a performance.\n",
    "\n",
    "- **`mean_squared_error`**: Calcula o erro quadrático médio (MSE) entre as previsões e os valores reais, ajudando a medir a qualidade do modelo de regressão."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
