{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto de **Machine Learning (ML)**, um **pipeline** é uma sequência de etapas que automatiza o fluxo de trabalho de um modelo de aprendizado de máquina. Ele envolve todas as fases necessárias para transformar dados brutos em um modelo treinado e em uma solução operacional. Em vez de executar manualmente cada tarefa, um pipeline organiza e automatiza as etapas do processo de ML.\n",
    "\n",
    "### Etapas típicas de um pipeline de Machine Learning:\n",
    "\n",
    "1. **Pré-processamento de dados**:\n",
    "   - Limpeza de dados (remover valores ausentes, tratamento de outliers).\n",
    "   - Normalização ou padronização de dados.\n",
    "   - Codificação de variáveis categóricas.\n",
    "   - Divisão dos dados em conjuntos de treinamento e teste.\n",
    "\n",
    "2. **Seleção de características**:\n",
    "   - Identificar e escolher as variáveis ou atributos mais relevantes para o modelo.\n",
    "\n",
    "3. **Treinamento do modelo**:\n",
    "   - Escolher e treinar um modelo de machine learning (como uma árvore de decisão, regressão linear, redes neurais, etc.).\n",
    "   - Ajuste de parâmetros (tuning) usando técnicas como validação cruzada.\n",
    "\n",
    "4. **Avaliação do modelo**:\n",
    "   - Testar o modelo em um conjunto de dados de validação ou teste.\n",
    "   - Calcular métricas de desempenho (como acurácia, precisão, recall, etc.).\n",
    "\n",
    "5. **Implantação do modelo**:\n",
    "   - Depois que o modelo é treinado e avaliado, ele é implantado em um ambiente de produção para fazer previsões em novos dados.\n",
    "\n",
    "6. **Monitoramento e manutenção**:\n",
    "   - Monitorar o desempenho do modelo ao longo do tempo para detectar deterioração (drift) e atualizar conforme necessário.\n",
    "\n",
    "### Vantagens do uso de um pipeline em ML:\n",
    "- **Automação**: Reduz a necessidade de intervenção manual, o que economiza tempo e reduz erros.\n",
    "- **Reprodutibilidade**: Permite que os experimentos sejam reproduzíveis com os mesmos dados e parâmetros.\n",
    "- **Eficiência**: Facilita o gerenciamento de diferentes etapas de processamento e treinamento de modelos.\n",
    "- **Facilidade de manutenção**: Quando há a necessidade de ajustar ou modificar uma parte do pipeline, as mudanças podem ser feitas de maneira mais isolada e controlada.\n",
    "\n",
    "No mundo real, ferramentas como **scikit-learn**, **TensorFlow**, **Kubeflow**, e **Apache Airflow** são comumente usadas para construir e gerenciar pipelines de Machine Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9556\n",
      "[1 0 2 1 2 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Carregar o conjunto de dados Iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Definir o pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),        # Etapa 1: Escalonamento dos dados\n",
    "    ('pca', PCA(n_components=2)),        # Etapa 2: PCA para redução de dimensionalidade\n",
    "    ('classifier', RandomForestClassifier())  # Etapa 3: Classificador Random Forest\n",
    "])\n",
    "\n",
    "# Treinar o pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar o desempenho no conjunto de teste\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(f\"Accuracy: {score:.4f}\")\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'classifier__n_estimators': 50, 'pca__n_components': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir o grid de parâmetros\n",
    "param_grid = {\n",
    "    'pca__n_components': [2, 3],  # Parâmetros para PCA\n",
    "    'classifier__n_estimators': [10, 50],  # Parâmetros para RandomForest\n",
    "}\n",
    "\n",
    "# Criar o GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "\n",
    "# Treinar o modelo com o GridSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar os melhores parâmetros\n",
    "print(\"Melhores parâmetros:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding\n",
    "\n",
    "O **One-Hot Encoding** é uma técnica usada para converter variáveis categóricas em uma representação numérica, permitindo que modelos de aprendizado de máquina as processem corretamente.\n",
    "\n",
    "## Como Funciona:\n",
    "1. **Entrada Categórica**: Você começa com uma variável que tem categorias distintas. Por exemplo, uma coluna com as categorias `[\"gato\", \"cachorro\", \"passarinho\"]`.\n",
    "   \n",
    "2. **Criação de Vetores**: Para cada categoria, cria-se um vetor de tamanho igual ao número total de categorias. Nesse vetor, uma posição será marcada como \"1\", indicando a presença dessa categoria, e as outras posições serão \"0\". Exemplo:\n",
    "   - \"gato\" → `[1, 0, 0]`\n",
    "   - \"cachorro\" → `[0, 1, 0]`\n",
    "   - \"passarinho\" → `[0, 0, 1]`\n",
    "\n",
    "3. **Resultado**: Agora, cada categoria é representada por um vetor binário único. Isso permite que a variável categórica seja manipulada por algoritmos de aprendizado de máquina, que requerem entradas numéricas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código é utilizado para realizar pré-processamento em um conjunto de dados, mais especificamente, para aplicar transformações diferentes às colunas numéricas e categóricas de um DataFrame X. Vamos explicar cada parte detalhadamente:\n",
    "\n",
    "### Importação das Bibliotecas:\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "```\n",
    "Aqui, são importados os módulos necessários para fazer o pré-processamento:\n",
    "\n",
    "- `ColumnTransformer`: permite aplicar transformações diferentes a diferentes subconjuntos de colunas de um DataFrame.\n",
    "- `StandardScaler`: é usado para normalizar as colunas numéricas, escalando-as para que tenham média 0 e desvio padrão 1.\n",
    "- `OneHotEncoder`: transforma variáveis categóricas em variáveis binárias (one-hot encoding).\n",
    "\n",
    "### Seleção de Features Numéricas:\n",
    "``` python\n",
    "num_features = X.select_dtypes(include='float64').columns\n",
    "num_transformer = StandardScaler()\n",
    "```\n",
    "- `num_features`: seleciona todas as colunas do DataFrame X que são do tipo numérico (float64).\n",
    "- `num_transformer`: define o transformador para as variáveis numéricas, que será o StandardScaler (para normalizar os dados numéricos).\n",
    "\n",
    "### Seleção de Features Categóricas:\n",
    "\n",
    "```python\n",
    "cat_features = X.select_dtypes(include='category').columns\n",
    "cat_transformer = OneHotEncoder(drop='if_binary')\n",
    "```\n",
    "- `cat_features`: seleciona todas as colunas do DataFrame X que são do tipo categórico (category).\n",
    "- `cat_transformer`: define o transformador para as variáveis categóricas, que será o OneHotEncoder (para realizar a codificação one-hot). O parâmetro drop='if_binary' faz com que, caso a variável categórica tenha apenas duas categorias (binária), ela seja codificada de forma otimizada, removendo uma das colunas geradas.\n",
    "\n",
    "### Criação do ColumnTransformer:\n",
    "\n",
    "```python\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_features),\n",
    "        ('cat', cat_transformer, cat_features),\n",
    "    ],\n",
    "    sparse_threshold=0,\n",
    ")\n",
    "```\t\n",
    "O `ColumnTransformer` é criado com as transformações para as variáveis numéricas e categóricas:\n",
    "\n",
    "- Para as variáveis numéricas, o transformador será o StandardScaler, que será aplicado às colunas especificadas em num_features.\n",
    "- Para as variáveis categóricas, o transformador será o OneHotEncoder, aplicado às colunas especificadas em cat_features.\n",
    "O parâmetro sparse_threshold=0 garante que a saída do ColumnTransformer não será uma matriz esparsa, ou seja, os dados serão retornados como um array denso, mesmo que haja grande número de colunas geradas (caso de OneHotEncoder).\n",
    "\n",
    "Aplicação do ColumnTransformer no conjunto de treinamento (X_train):\n",
    "\n",
    "```python\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "Aqui, o fit_transform é aplicado ao DataFrame X_train:\n",
    "```\t\n",
    "\n",
    "- `fit`: ajusta o transformador às colunas do conjunto de dados de treinamento (X_train), aprendendo os parâmetros (como a média e desvio padrão para normalização, ou as categorias para one-hot encoding).\n",
    "- `transform`: aplica as transformações aprendidas ao conjunto de dados X_train.\n",
    "O resultado, X_train_processed, é o DataFrame ou array transformado com as colunas numéricas normalizadas e as categóricas codificadas.\n",
    "\n",
    "### Resumo:\n",
    "Esse código prepara os dados de X para um modelo de machine learning, separando e transformando colunas numéricas e categóricas de forma diferente:\n",
    "\n",
    "As colunas numéricas são normalizadas.\n",
    "As colunas categóricas são codificadas usando one-hot encoding.\n",
    "O ColumnTransformer aplica essas transformações de forma eficiente e pode ser utilizado em diferentes etapas do pipeline de pré-processamento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
